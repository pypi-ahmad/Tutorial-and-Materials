{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7fd24f7",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dfbfd6",
   "metadata": {},
   "source": [
    "Ready to turn your raw data into a delicious feast for your machine learning model? Then grab your apron, put on your chef's hat, and let's get cooking! We'll be whipping up a data preprocessing template that will make sure your data is neat, tidy, and ready to be served to your ML model.\n",
    "\n",
    "The steps in data processing are as follows:\n",
    "\n",
    "Importing Libraries\n",
    "Loading Data\n",
    "Handling Missing Data\n",
    "Handling Outliers\n",
    "Data Normalization\n",
    "Splitting Data\n",
    "One-Hot Encoding\n",
    "Feature Selection\n",
    "Training the Model\n",
    "Evaluating the Model.\n",
    "\n",
    "\n",
    "We will look into each step in detail in this notebook.Just follow each step and you'll have a fantastic meal ready in no time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9456e197",
   "metadata": {},
   "source": [
    "## Step 1 : Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052a2967",
   "metadata": {},
   "source": [
    "Importing Libraries: The first step is to import the necessary libraries, including numpy, pandas, matplotlib, and various preprocessing and model selection modules from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d75b278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb97a0ef",
   "metadata": {},
   "source": [
    "## Step 2 : Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ec9cd",
   "metadata": {},
   "source": [
    "Load the Data: Next, we load the raw data into a pandas dataframe using the read_csv method. The raw data should be in a .csv file format, and the path/to/raw_data.csv should be replaced with the actual file path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c5f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"path/to/raw_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e31f9",
   "metadata": {},
   "source": [
    "## Step 3 : Handle Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cbf034",
   "metadata": {},
   "source": [
    "After loading the data, we need to handle any missing values. Missing values can be handled in several ways, such as dropping the rows containing missing values, filling missing values with mean or median, etc. We can use the dropna or fillna method to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b68c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Filling missing values with mean\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "# Filling missing values with median\n",
    "data.fillna(data.median(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3656e5a",
   "metadata": {},
   "source": [
    "## Step 4 : Handle Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e23516",
   "metadata": {},
   "source": [
    "Outliers can negatively impact the performance of the machine learning model. Therefore, it is important to detect and remove outliers. One common method to detect outliers is to calculate the interquartile range (IQR) and remove any data points outside of a specified range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa432d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting and removing outliers\n",
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "data = data[~((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2dab48",
   "metadata": {},
   "source": [
    "## Step 5 : Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015a565b",
   "metadata": {},
   "source": [
    "Machine learning algorithms perform better when the data is in a standardized format. Therefore, we need to normalize the data. There are two common normalization techniques, min-max scaling, and standardization. Min-max scaling scales the data between 0 and 1, while standardization scales the data so that it has a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f4678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6893965",
   "metadata": {},
   "source": [
    "## Step 6 : Split the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602f3fed",
   "metadata": {},
   "source": [
    "The next step is to split the data into training and test sets. The training set is used to train the machine learning model, and the test set is used to evaluate its performance. The split can be done using the train_test_split method, which takes in the data and target variable, as well as the percentage of data to be used as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb287a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bcc536",
   "metadata": {},
   "source": [
    "## Step 7 : One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcc6550",
   "metadata": {},
   "source": [
    "Some machine learning algorithms can only handle numerical data, therefore we need to convert categorical data into numerical data. One-hot encoding is a common method to convert categorical data into numerical data. It creates a new column for each unique category and assigns a binary value of 1 or 0 to indicate the presence or absence of the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6bfdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding\n",
    "encoder = OneHotEncoder()\n",
    "data = encoder.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e889bb",
   "metadata": {},
   "source": [
    "## Step 8 : Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f91863a",
   "metadata": {},
   "source": [
    "Not all features are equally important for the machine learning model. By selecting only the most important features, we can improve the performance of the model and reduce overfitting. Feature selection can be done using several methods, such as feature importance, correlation matrix, or recursive feature elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0823e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Correlation Matrix\n",
    "correlation = data.corr()\n",
    "\n",
    "# Recursive Feature Elimination\n",
    "from sklearn.feature_selection import RFE\n",
    "model = RandomForestClassifier()\n",
    "rfe = RFE(model, n_features_to_select)\n",
    "fit = rfe.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ac56ea",
   "metadata": {},
   "source": [
    "## Step 9 : Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ef4662",
   "metadata": {},
   "source": [
    "Once the data is preprocessed, we can train the machine learning model using the training set. Any machine learning algorithm can be used at this stage, such as logistic regression, decision trees, random forests, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85476a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc97679",
   "metadata": {},
   "source": [
    "## Step 10 : Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e32c2e4",
   "metadata": {},
   "source": [
    "Finally, we evaluate the performance of the machine learning model using the test set. Evaluation metrics, such as accuracy, precision, recall, and F1 score, can be used to measure the performance of the model. The results of the evaluation can be used to make improvements to the model, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff98674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88126018",
   "metadata": {},
   "source": [
    "Congratulations! By following this data preprocessing template, you've taken a crucial step towards creating an accurate and powerful machine learning model. Keep experimenting and fine-tuning your approach, and who knows what insights and breakthroughs you'll uncover. Happy data processing!\n",
    "\n",
    "Thanks,\n",
    "Team Tensorcode.io"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
